        __end__([<p>__end__</p>]):::last

(YouTube Multimodal RAG Pipeline) E:\YouTube Multimodal RAG Pipeline>uv run streamlit run app.py

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.1.6:8501

---FETCHING METADATA & MEDIA---
---STARTING AUDIO DOWNLOAD: https://youtu.be/6EJaHBJhwDs?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7---
[YoutubeYtBe] Extracting URL: https://youtu.be/6EJaHBJhwDs?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7 
[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=6EJaHBJhwDs&list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7&feature=youtu.be 
[youtube:tab] Downloading just the video 6EJaHBJhwDs because of --no-playlist      
[youtube] Extracting URL: https://www.youtube.com/watch?v=6EJaHBJhwDs 
[youtube] 6EJaHBJhwDs: Downloading webpage
WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one
[youtube] 6EJaHBJhwDs: Downloading android sdkless player API JSON
[youtube] 6EJaHBJhwDs: Downloading web safari player API JSON 
WARNING: [youtube] 6EJaHBJhwDs: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details
[youtube] 6EJaHBJhwDs: Downloading m3u8 information
WARNING: [youtube] 6EJaHBJhwDs: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details
[info] 6EJaHBJhwDs: Downloading 1 format(s): 251 
[download] Destination: downloads\audio 
[download] 100% of   30.86MiB in 00:00:04 at 7.42MiB/s
[ExtractAudio] Destination: downloads\audio.mp3 
Deleting original file downloads\audio (pass -k to keep) 
---TRANSCRIBING WITH GROQ WHISPER (Cloud API)...---
---COMPLETED TRANSCRIPTION: 949 segments.---
---STARTING VIDEO DOWNLOAD FOR FRAMES---
[YoutubeYtBe] Extracting URL: https://youtu.be/6EJaHBJhwDs?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7
[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=6EJaHBJhwDs&list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7&feature=youtu.be
[youtube:tab] Downloading just the video 6EJaHBJhwDs because of --no-playlist      
[youtube] Extracting URL: https://www.youtube.com/watch?v=6EJaHBJhwDs
[youtube] 6EJaHBJhwDs: Downloading webpage
WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one
[youtube] 6EJaHBJhwDs: Downloading android sdkless player API JSON
[youtube] 6EJaHBJhwDs: Downloading web safari player API JSON 
WARNING: [youtube] 6EJaHBJhwDs: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details
[youtube] 6EJaHBJhwDs: Downloading m3u8 information
WARNING: [youtube] 6EJaHBJhwDs: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details
[info] 6EJaHBJhwDs: Downloading 1 format(s): 136 
[download] Destination: downloads\temp_video.mp4 
[download] 100% of   43.72MiB in 00:00:06 at 7.04MiB/s
---STARTING FRAME EXTRACTION (OpenCV)...---
---COMPLETED: Extracted 473 frames and 949 audio segments.---
---ANALYZING VISUALS WITH GEMINI 1.5 FLASH---
  Gemini describing frame at 0.00s...
────────────────────────── Traceback (most recent call last) ───────────────────────────
  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_google_genai\chat_models.py:3040 in   

  _generate                                                                        

                                                                                   

    3037 │   │   │   **kwargs,                                                     

    3038 │   │   )                                                                 

    3039 │   │   try:                                                              

  ❱ 3040 │   │   │   response: GenerateContentResponse = self.client.models.generate_c
    3041 │   │   │   │   **request,                                                

    3042 │   │   │   )                                                             

    3043 │   │   except ClientError as e:                                          

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\models.py:5215 in generate_content 

                                                                                   

    5212 │   i = 0                                                                 

    5213 │   while remaining_remote_calls_afc > 0:                                 

    5214 │     i += 1                                                              

  ❱ 5215 │     response = self._generate_content(                                  

    5216 │   │     model=model, contents=contents, config=parsed_config            

    5217 │     )                                                                   

    5218                                                                           

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\models.py:3997 in _generate_content

                                                                                   

    3994 │   request_dict = _common.convert_to_dict(request_dict)                  

    3995 │   request_dict = _common.encode_unserializable_types(request_dict)      

    3996 │                                                                         

  ❱ 3997 │   response = self._api_client.request(                                  

    3998 │   │   'post', path, request_dict, http_options                          

    3999 │   )                                                                     

    4000                                                                           

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\_api_client.py:1375 in request     

                                                                                   

    1372 │   http_request = self._build_request(                                   

    1373 │   │   http_method, path, request_dict, http_options                     

    1374 │   )                                                                     

  ❱ 1375 │   response = self._request(http_request, http_options, stream=False)    

    1376 │   response_body = (                                                     

    1377 │   │   response.response_stream[0] if response.response_stream else ''   

    1378 │   )                                                                     

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\_api_client.py:1209 in _request    

                                                                                   

    1206 │     if parameter_model.retry_options:                                   

    1207 │   │   retry_kwargs = retry_args(parameter_model.retry_options)          

    1208 │   │   retry = tenacity.Retrying(**retry_kwargs)                         

  ❱ 1209 │   │   return retry(self._request_once, http_request, stream)  # type: ignor
    1210 │                                                                         

    1211 │   return self._retry(self._request_once, http_request, stream)  # type: ign
    1212                                                                           

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\.venv\Lib\site-packages\tenacity\__init__.py:477
  in __call__                                                                      

                                                                                   

    474 │   │                                                                      

    475 │   │   retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwar
    476 │   │   while True:                                                        

  ❱ 477 │   │   │   do = self.iter(retry_state=retry_state)                        

    478 │   │   │   if isinstance(do, DoAttempt):                                  

    479 │   │   │   │   try:                                                       

    480 │   │   │   │   │   result = fn(*args, **kwargs)                           

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\.venv\Lib\site-packages\tenacity\__init__.py:378
  in iter                                                                          

                                                                                   

    375 │   │   self._begin_iter(retry_state)                                      

    376 │   │   result = None                                                      

    377 │   │   for action in self.iter_state.actions:                             

  ❱ 378 │   │   │   result = action(retry_state)                                   

    379 │   │   return result                                                      

    380 │                                                                          

    381 │   def _begin_iter(self, retry_state: "RetryCallState") -> None:  # noqa  

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\.venv\Lib\site-packages\tenacity\__init__.py:400
  in <lambda>                                                                      

                                                                                   

    397 │                                                                          

    398 │   def _post_retry_check_actions(self, retry_state: "RetryCallState") -> None
    399 │   │   if not (self.iter_state.is_explicit_retry or self.iter_state.retry_run
  ❱ 400 │   │   │   self._add_action_func(lambda rs: rs.outcome.result())          

    401 │   │   │   return                                                         

    402 │   │                                                                      

    403 │   │   if self.after is not None:                                         

                                                                                   

  E:\anaconda\Anaconda3_installation\Lib\concurrent\futures\_base.py:449 in result 

                                                                                   

    446 │   │   │   │   if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:     

    447 │   │   │   │   │   raise CancelledError()                                 

    448 │   │   │   │   elif self._state == FINISHED:                              

  ❱ 449 │   │   │   │   │   return self.__get_result()                             

    450 │   │   │   │                                                              

    451 │   │   │   │   self._condition.wait(timeout)                              

    452                                                                            

                                                                                   

  E:\anaconda\Anaconda3_installation\Lib\concurrent\futures\_base.py:401 in        

  __get_result                                                                     

                                                                                   

    398 │   def __get_result(self):                                                

    399 │   │   if self._exception:                                                

    400 │   │   │   try:                                                           

  ❱ 401 │   │   │   │   raise self._exception                                      

    402 │   │   │   finally:                                                       

    403 │   │   │   │   # Break a reference cycle with the exception in self._exceptio
    404 │   │   │   │   self = None                                                

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\.venv\Lib\site-packages\tenacity\__init__.py:480
  in __call__                                                                      

                                                                                   

    477 │   │   │   do = self.iter(retry_state=retry_state)                        

    478 │   │   │   if isinstance(do, DoAttempt):                                  

    479 │   │   │   │   try:                                                       

  ❱ 480 │   │   │   │   │   result = fn(*args, **kwargs)                           

    481 │   │   │   │   except BaseException:  # noqa: B902                        

    482 │   │   │   │   │   retry_state.set_exception(sys.exc_info())  # type: ignore[
    483 │   │   │   │   else:                                                      

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\_api_client.py:1188 in _request_once
                                                                                   

    1185 │   │     content=data,                                                   

    1186 │   │     timeout=http_request.timeout,                                   

    1187 │     )                                                                   

  ❱ 1188 │     errors.APIError.raise_for_response(response)                        

    1189 │     return HttpResponse(                                                

    1190 │   │     response.headers, response if stream else [response.text]       

    1191 │     )                                                                   

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\errors.py:121 in raise_for_response

                                                                                   

    118 │   else:                                                                  

    119 │     response_json = response.body_segments[0].get('error', {})           

    120 │                                                                          

  ❱ 121 │   cls.raise_error(response.status_code, response_json, response)         

    122                                                                            

    123   @classmethod                                                             

    124   def raise_error(                                                         

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\google\genai\errors.py:146 in raise_error       

                                                                                   

    143 │     APIError: For other error status codes.                              

    144 │   """                                                                    

    145 │   if 400 <= status_code < 500:                                           

  ❱ 146 │     raise ClientError(status_code, response_json, response)              

    147 │   elif 500 <= status_code < 600:                                         

    148 │     raise ServerError(status_code, response_json, response)              

    149 │   else:                                                                  

────────────────────────────────────────────────────────────────────────────────────────
ClientError: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not 
valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details':     
[{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID',

'domain': 'googleapis.com', 'metadata': {'service':
'generativelanguage.googleapis.com'}}, {'@type':
'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API
key not valid. Please pass a valid API key.'}]}}
[NOTE] During task with name 'multimodal_processor' and id
'6696a6db-5a80-de7c-9cb7-6507c596a6f1'

The above exception was the direct cause of the following exception:

────────────────────────── Traceback (most recent call last) ───────────────────────────
  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\streamlit\runtime\scriptrunner\exec_code.py:129 in
  exec_func_with_error_handling                                                    

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py:671
  in code_to_exec                                                                  

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\app.py:117 in <module>                        

                                                                                   

    114 │   │   │   st.chat_message("assistant").write(response)                   

    115                                                                            

    116 if __name__ == "__main__":                                                 

  ❱ 117 │   main()                                                                 

    118                                                                            

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\app.py:66 in main                             

                                                                                   

     63 │   │   │   │   │   final_state = {}                                       

     64 │   │   │   │   │   nodes = list(node_messages.keys())                     

     65 │   │   │   │   │                                                          

  ❱  66 │   │   │   │   │   for output in langgraph_app.stream(inputs):            

     67 │   │   │   │   │   │   for key, value in output.items():                  

     68 │   │   │   │   │   │   │   msg = node_messages.get(key, f"Processing {key}...
     69 │   │   │   │   │   │   │   st.write(msg)                                  

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langgraph\pregel\main.py:2643 in stream         

                                                                                   

    2640 │   │   │   │   while loop.tick():                                        

    2641 │   │   │   │   │   for task in loop.match_cached_writes():               

    2642 │   │   │   │   │   │   loop.output_writes(task.id, task.writes, cached=True)
  ❱ 2643 │   │   │   │   │   for _ in runner.tick(                                 

    2644 │   │   │   │   │   │   [t for t in loop.tasks.values() if not t.writes], 

    2645 │   │   │   │   │   │   timeout=self.step_timeout,                        

    2646 │   │   │   │   │   │   get_waiter=get_waiter,                            

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langgraph\pregel\_runner.py:167 in tick         

                                                                                   

    164 │   │   elif len(tasks) == 1 and timeout is None and get_waiter is None:   

    165 │   │   │   t = tasks[0]                                                   

    166 │   │   │   try:                                                           

  ❱ 167 │   │   │   │   run_with_retry(                                            

    168 │   │   │   │   │   t,                                                     

    169 │   │   │   │   │   retry_policy,                                          

    170 │   │   │   │   │   configurable={                                         

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langgraph\pregel\_retry.py:42 in run_with_retry 

                                                                                   

     39 │   │   │   # clear any writes from previous attempts                      

     40 │   │   │   task.writes.clear()                                            

     41 │   │   │   # run the task                                                 

  ❱  42 │   │   │   return task.proc.invoke(task.input, config)                    

     43 │   │   except ParentCommand as exc:                                       

     44 │   │   │   ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]               

     45 │   │   │   cmd = exc.args[0]                                              

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langgraph\_internal\_runnable.py:656 in invoke  

                                                                                   

    653 │   │   │   │   │   │   run = None                                         

    654 │   │   │   │   │   # run in context                                       

    655 │   │   │   │   │   with set_config_context(config, run) as context:       

  ❱ 656 │   │   │   │   │   │   input = context.run(step.invoke, input, config, **kwar
    657 │   │   │   │   else:                                                      

    658 │   │   │   │   │   input = step.invoke(input, config)                     

    659 │   │   # finish the root run                                              

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langgraph\_internal\_runnable.py:400 in invoke  

                                                                                   

    397 │   │   │   else:                                                          

    398 │   │   │   │   run_manager.on_chain_end(ret)                              

    399 │   │   else:                                                              

  ❱ 400 │   │   │   ret = self.func(*args, **kwargs)                               

    401 │   │   if self.recurse and isinstance(ret, Runnable):                     

    402 │   │   │   return ret.invoke(input, config)                               

    403 │   │   return ret                                                         

                                                                                   

  E:\YouTube Multimodal RAG Pipeline\graph.py:73 in multimodal_processor           

                                                                                   

     70 │   │   │   {"type": "text", "text": "Describe the visual content of this fram
     71 │   │   │   {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base6
     72 │   │   ])                                                                 

  ❱  73 │   │   res = llm.invoke([msg])                                            

     74 │   │   event["description"] = res.content                                 

     75 │   │   processed_events.append(event)                                     

     76                                                                            

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_google_genai\chat_models.py:2535 in   

  invoke                                                                           

                                                                                   

    2532 │   │   │   │   msg = "Tools are already defined.code_execution tool can't be
    2533 │   │   │   │   raise ValueError(msg)                                     

    2534 │   │                                                                     

  ❱ 2535 │   │   return super().invoke(input, config, stop=stop, **kwargs)         

    2536 │                                                                         

    2537 │   def _get_ls_params(                                                   

    2538 │   │   self, stop: list[str] | None = None, **kwargs: Any                

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:402
  in invoke                                                                        

                                                                                   

     399 │   │   │   "AIMessage",                                                  

     400 │   │   │   cast(                                                         

     401 │   │   │   │   "ChatGeneration",                                         

  ❱  402 │   │   │   │   self.generate_prompt(                                     

     403 │   │   │   │   │   [self._convert_input(input)],                         

     404 │   │   │   │   │   stop=stop,                                            

     405 │   │   │   │   │   callbacks=config.get("callbacks"),                    

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:1121
  in generate_prompt                                                               

                                                                                   

    1118 │   │   **kwargs: Any,                                                    

    1119 │   ) -> LLMResult:                                                       

    1120 │   │   prompt_messages = [p.to_messages() for p in prompts]              

  ❱ 1121 │   │   return self.generate(prompt_messages, stop=stop, callbacks=callbacks,
    1122 │                                                                         

    1123 │   @override                                                             

    1124 │   async def agenerate_prompt(                                           

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:931
  in generate                                                                      

                                                                                   

     928 │   │   for i, m in enumerate(input_messages):                            

     929 │   │   │   try:                                                          

     930 │   │   │   │   results.append(                                           

  ❱  931 │   │   │   │   │   self._generate_with_cache(                            

     932 │   │   │   │   │   │   m,                                                

     933 │   │   │   │   │   │   stop=stop,                                        

     934 │   │   │   │   │   │   run_manager=run_managers[i] if run_managers else None
                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:1233
  in _generate_with_cache                                                          

                                                                                   

    1230 │   │   │   │   chunks.append(chunk)                                      

    1231 │   │   │   result = generate_from_stream(iter(chunks))                   

    1232 │   │   elif inspect.signature(self._generate).parameters.get("run_manager"):
  ❱ 1233 │   │   │   result = self._generate(                                      

    1234 │   │   │   │   messages, stop=stop, run_manager=run_manager, **kwargs    

    1235 │   │   │   )                                                             

    1236 │   │   else:                                                             

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_google_genai\chat_models.py:3044 in   

  _generate                                                                        

                                                                                   

    3041 │   │   │   │   **request,                                                

    3042 │   │   │   )                                                             

    3043 │   │   except ClientError as e:                                          

  ❱ 3044 │   │   │   _handle_client_error(e, request)                              

    3045 │   │                                                                     

    3046 │   │   return _response_to_result(response)                              

    3047                                                                           

                                                                                   

  E:\YouTube Multimodal RAG                                                        

  Pipeline\.venv\Lib\site-packages\langchain_google_genai\chat_models.py:145 in    

  _handle_client_error                                                             

                                                                                   

     142 │   """                                                                   

     143 │   model_name = request.get("model", "unknown")                          

     144 │   msg = f"Error calling model '{model_name}' ({e.status}): {e}"         

  ❱  145 │   raise ChatGoogleGenerativeAIError(msg) from e                         

     146                                                                           

     147                                                                           

     148 def _get_default_model_profile(model_name: str) -> ModelProfile:          

────────────────────────────────────────────────────────────────────────────────────────
ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (INVALID_ARGUMENT):
400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass
a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type':
'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 
'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}},   
{'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US',    
'message': 'API key not valid. Please pass a valid API key.'}]}}
[NOTE] During task with name 'multimodal_processor' and id
'6696a6db-5a80-de7c-9cb7-6507c596a6f1'